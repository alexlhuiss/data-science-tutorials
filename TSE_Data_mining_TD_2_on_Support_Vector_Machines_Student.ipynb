{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexlhuiss/data-science-tutorials/blob/master/TSE_Data_mining_TD_2_on_Support_Vector_Machines_Student.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CW4SWI6YaA3q"
      },
      "source": [
        "# TD on Support Vector Machines or SVMs (part 1)\n",
        "\n",
        "### Usage of this notebook\n",
        "- __run a cell__ with **_ctrl-enter_** or **_shift-enter_**\n",
        "- __use the command palette__ with **_ctrl-shift-P_** to find more complex commands\n",
        "\n",
        "Use it referably with __Edge__ or __Chrome__\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nteEjjzPNuM8"
      },
      "source": [
        "# Some typical scientific & data manipulation packages\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Keras, deep learning library, used here only to load the MNIST dataset\n",
        "from keras.datasets import mnist\n",
        "\n",
        "# Imports from sklearn package: data generation, svm module & metrics\n",
        "from sklearn import svm\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import confusion_matrix, roc_auc_score, accuracy_score, mean_squared_error\n",
        "\n",
        "\n",
        "# Visualization packages\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cw4NdH5cNVIT"
      },
      "source": [
        "# **Exercise 1. Implement your own simple linear SVM classifier**\n",
        "\n",
        "#### Let's write a simplified code for SVMs on a random dataset\n",
        "- 1.1. Generate random dataset & visualize it (done)\n",
        "- 1.2. Write a generic code for SVM training (fit)\n",
        "- 1.3. Play around with dataset & algorithm parameters. Visualize optimum boundary.\n",
        "- 1.4. Let's try the multiclass classification [optional]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Initialization"
      ],
      "metadata": {
        "id": "qmJ6QTckdRN5"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2yCkkmb_S3h"
      },
      "source": [
        "# Generate random dataset\n",
        "X, Y = make_blobs(n_samples=100, centers=2, random_state=1, cluster_std=0.5)\n",
        "\n",
        "# Visualize the dataset\n",
        "plt.scatter(X[:,0], X[:,1], c=Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqulvDKAkMXj"
      },
      "source": [
        "### 1.2 Simple linear Support Vector Machine code"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SVM:\n",
        "\n",
        "  # Initialization of algorithm parameters\n",
        "  def __init__(self, learning_rate=0.001, lambda_param=0.01, n_iters=1000):\n",
        "    self.lr = learning_rate\n",
        "    self.lambda_param = lambda_param\n",
        "    self.n_iters = n_iters\n",
        "    self.w = None\n",
        "    self.b = None\n",
        "\n",
        "  def fit(self, X, Y):\n",
        "    n_samples, n_features = ??? # samples => lines, features => columns of X\n",
        "\n",
        "    # Change class label from 0 to -1\n",
        "    # (6 different ways: https://datagy.io/python-replace-item-in-list/)\n",
        "    Y = ???\n",
        "\n",
        "    # Initialize weights and bias to zero for start, remember that w is a vector\n",
        "    # so you must use np.zeros(dim), b is a scalar\n",
        "    self.w = ???\n",
        "    self.b = ???\n",
        "\n",
        "    # Iterate over the predefined amount of iterations on __init__ of your class\n",
        "    for iter in range(self.n_iters):\n",
        "\n",
        "      # for each point Xi in your dataset\n",
        "      for idx, Xi in enumerate(X):\n",
        "\n",
        "        # Test your condition: is Yi(Xi.W - b) >= 1?\n",
        "        condition = ???\n",
        "\n",
        "        # If the condition is satisfied, then update the weights accordingly\n",
        "        if condition:\n",
        "          self.w -= ???\n",
        "        else:\n",
        "          self.w -= ???\n",
        "          # Update the bias\n",
        "          self.b -= ???\n",
        "\n",
        "  def predict(self, X):\n",
        "    # y = wX - b\n",
        "    y_pred = ???\n",
        "    # return the sign of this approximation\n",
        "    return ???\n",
        "\n",
        "clf = SVM()\n",
        "clf.fit(???)\n",
        "print(clf.w, clf.b)\n",
        "# predictions = clf.predict(my_point) # You can take random points and make a prediction"
      ],
      "metadata": {
        "id": "WJduRrdLCReI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJR2vTCwxVb3"
      },
      "source": [
        "### 1.3 Play with#\n",
        "- the input dataset\n",
        "  - nb of samples\n",
        "  - cluster std\n",
        "  - random state\n",
        "- the SVM algorithm hyperparameters\n",
        "  - learning rate\n",
        "  - lambda\n",
        "  - nb iterations\n",
        "  & launch the visualization code below to view the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nPsfypKlSUX"
      },
      "source": [
        "def visualize_svm():\n",
        "  def get_hyperplane_value(x, w, b, offset):\n",
        "      return (-w[0] * x + b + offset) / w[1]\n",
        "\n",
        "  fig = plt.figure()\n",
        "  ax = fig.add_subplot(1, 1, 1)\n",
        "  plt.scatter(X[:,0], X[:,1], c=Y, s=50)\n",
        "\n",
        "  x0_1 = np.amin(X[:,0])\n",
        "  x0_2 = np.amax(X[:,0])\n",
        "\n",
        "  x1_1 = get_hyperplane_value(x0_1, clf.w, clf.b, 0)\n",
        "  x1_2 = get_hyperplane_value(x0_2, clf.w, clf.b, 0)\n",
        "\n",
        "  x1_1_m = get_hyperplane_value(x0_1, clf.w, clf.b, -1)\n",
        "  x1_2_m = get_hyperplane_value(x0_2, clf.w, clf.b, -1)\n",
        "\n",
        "  x1_1_p = get_hyperplane_value(x0_1, clf.w, clf.b, 1)\n",
        "  x1_2_p = get_hyperplane_value(x0_2, clf.w, clf.b, 1)\n",
        "\n",
        "  ax.plot([x0_1, x0_2], [x1_1, x1_2], \"k\")\n",
        "  ax.plot([x0_1, x0_2], [x1_1_m, x1_2_m], \"b--\")\n",
        "  ax.plot([x0_1, x0_2], [x1_1_p, x1_2_p], \"y--\")\n",
        "\n",
        "  x1_min = np.amin(X[:,1])\n",
        "  x1_max = np.amax(X[:,1])\n",
        "  ax.set_ylim([x1_min - 3, x1_max + 3])\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "visualize_svm()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4 Multiclass classifier [OPTIONAL]\n",
        "\n",
        "You can try to implement the one-vs-one or the one-vs-rest method using the SVM class above"
      ],
      "metadata": {
        "id": "zTqlVKyCvB0a"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fb_UEXENUoJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFVSgQ5jQm-i"
      },
      "source": [
        "# **Exercise 2. Multi-class classification problem - Support Vector Classifier (SVC)**\n",
        "\n",
        "#### Let's test SVMs on images of the MNIST dataset\n",
        "First steps: loading the data set, displaying images & input formatting have been done for you. You will have to focus on:\n",
        "- 2.4. Training your SVM (select best kernel and parameters)\n",
        "- 2.5. Making predictions\n",
        "- 2.6. Measuring its accuracy\n",
        "- 2.7. [Optional] Compare your results with the ones obtained with your handmade neural network (do you remember!?)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKJvcu-xV24p"
      },
      "source": [
        "### 2.1. Load the data set (done)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yikVVzOe-jI"
      },
      "source": [
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(Y_train.shape)\n",
        "print(Y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAB5AOoVWIdH"
      },
      "source": [
        "### 2.2. Display images (done)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYe_OTYsH6MF"
      },
      "source": [
        "# 2.2.1\n",
        "imagesfig = plt.figure()\n",
        "for i in range(9):\n",
        "  plt.subplot(3,3,i+1)\n",
        "  plt.tight_layout()\n",
        "  plt.imshow(X_train[i], cmap='gray', interpolation='none')\n",
        "  plt.title(\"Digit: {}\".format(Y_train[i]))\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRKzF13494dI"
      },
      "source": [
        "# 2.2.2 What input vector & labels look like for a given image\n",
        "# Re-run several times to display several images\n",
        "# Choose a train image at random\n",
        "i = np.random.randint(0,X_train.shape[0]-1)\n",
        "print(\"Input vector: {} \".format(X_train[13, 13]))\n",
        "\n",
        "# Display image\n",
        "plt.imshow(1.0-X_train[i,:].reshape(28,28),cmap=\"gray\")\n",
        "plt.show()\n",
        "\n",
        "# Display label\n",
        "print(\"Label: {}\".format(Y_train[i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcq06pdBikPf"
      },
      "source": [
        "### 2.3. Input data reshaping (done)\n",
        "\n",
        "Reshape MNIST inputs to vector of length 784 (28*28) and scale to the [0-1] range"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jq7dx1DIikk2"
      },
      "source": [
        "img_rows, img_cols = 28, 28\n",
        "X_train = X_train.reshape(X_train.shape[0], img_rows * img_cols) / 255\n",
        "X_test = X_test.reshape(X_test.shape[0], img_rows * img_cols) / 255"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5InDgpPfPaG"
      },
      "source": [
        "### 2.4. Train your multiclass SVM classifier\n",
        "Or use sklearn library: https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
        "\n",
        "Note that we are predicting a class among: 0, 1, 2, 3, 4, 5, 6, 7, 8 & 9.\n",
        "\n",
        "\n",
        "/!\\ Please, don't take the entire dataset (take 1000 to 10000 samples otherwise you'll never get the result !)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Wfw58--fOum"
      },
      "source": [
        "model = ???\n",
        "model.fit(???)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4OOhlinjTgZ"
      },
      "source": [
        "### 2.5. Make a prediction for a random image of the test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhxGxT5NM2jn"
      },
      "source": [
        "# Re-run several times to test several images\n",
        "\n",
        "# Choose a test image at random\n",
        "i = np.random.randint(0, X_test.shape[0]-1)\n",
        "\n",
        "# Display image\n",
        "plt.imshow(1.0-X_test[i,:].reshape(28, 28),cmap=\"gray\")\n",
        "plt.show()\n",
        "\n",
        "# Predict the value\n",
        "single_prediction = model.predict([X_test[i,:]])\n",
        "\n",
        "# Display label\n",
        "print(\"Predicted label: {}\\nActual label: {} \".format(single_prediction[0],\n",
        "                                                      Y_test[i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmBncemRjblk"
      },
      "source": [
        "### 2.6. Plot the confusion matrix & measure the accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vt5StboxM9YJ"
      },
      "source": [
        "# Measure accuracy on a subset of the test set which was not used during training\n",
        "Y_predicted = model.predict(X_test[:1000])\n",
        "\n",
        "# Set the confusion matrix & display it\n",
        "cf_matrix = confusion_matrix(Y_test[:1000], Y_predicted)\n",
        "\n",
        "ax = sns.heatmap(cf_matrix, annot=True)\n",
        "\n",
        "ax.set_title('Confusion Matrix with labels\\n');\n",
        "ax.set_xlabel('Prediction')\n",
        "ax.set_ylabel('Actual');\n",
        "ax.xaxis.set_ticklabels([i for i in range(10)])\n",
        "ax.yaxis.set_ticklabels([i for i in range(10)])\n",
        "\n",
        "## Display the visualization of the Confusion Matrix.\n",
        "plt.show()\n",
        "\n",
        "# Compute the accuracy\n",
        "print(\"Accuracy=\", accuracy_score(Y_test[:1000], Y_predicted))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIem1i2GIST9"
      },
      "source": [
        "# Observe the pictures badly classified to check if you could have done better:\n",
        "for i in range(100):\n",
        "  if Y_test[i] != Y_predicted[i]:\n",
        "    # Display image\n",
        "    plt.imshow(1.0-X_test[i,:].reshape(28, 28),cmap=\"gray\")\n",
        "    plt.show()\n",
        "\n",
        "    # Display label\n",
        "    print(\"Predicted label: {}\\nActual label: {} \".format(Y_predicted[i],\n",
        "                                                          Y_test[i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xN1A6zuhU99c"
      },
      "source": [
        "# **Exercise 3. Simple regression problem - Support Vector Regressor (SVR)**\n",
        "\n",
        "#### Let's try SVMs on the prediction of fishes' weight based on their size characteristics\n",
        "- 3.1. Load the dataset and split into train & test (done): it can be found [here](https://www.kaggle.com/aungpyaeap/fish-market?select=Fish.csv)\n",
        "- 3.2. Do some exploration & transformation\n",
        "- 3.3. Split dataset into train & test\n",
        "- 3.4. Train your SVR (use sklearn.svm.SVR() function)\n",
        "- 3.5. Measure the accuracy of your model\n",
        "- 3.6. Improve your model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Get the current location of your notebook\n",
        "print(os.getcwd())\n",
        "\n",
        "# Connect to your drive (it will ask you your Google account)\n",
        "drive.mount(os.getcwd()+'/drive')\n",
        "\n",
        "# Navigate and list content of your Drive folders\n",
        "print(os.listdir('drive/MyDrive/???'))"
      ],
      "metadata": {
        "id": "pwofVe-VyCbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Load the dataset"
      ],
      "metadata": {
        "id": "MX_jRo2gdYrv"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4HSGg4jSXwh"
      },
      "source": [
        "dataset = pd.read_csv(\"???path???/Fish.csv\")\n",
        "dataset.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Explore the dataset"
      ],
      "metadata": {
        "id": "rgNOHSrVddSp"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFJuMKsMob6M"
      },
      "source": [
        "# 3.2.1 View distribution of the fish species\n",
        "species = dataset.Species.unique()\n",
        "count = [len(dataset.loc[dataset.Species==species])\n",
        "         for species in dataset.Species.unique()]\n",
        "plt.bar(species, count)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNkzMLzzTvzy"
      },
      "source": [
        "# 3.2.2 Plot relationships between variables\n",
        "df = dataset.loc[dataset.Species==\"Perch\"]\n",
        "plt.scatter(dataset[\"Height\"], dataset[\"Weight\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXllYqT0uIjo"
      },
      "source": [
        "# 3.2.3 Print the correlation matrix\n",
        "Corr_Matrix = round(df.corr(),2)\n",
        "print(Corr_Matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWBEy3aPtJoN"
      },
      "source": [
        "# 3.2.4 One hot encode the only categorical column of the dataset (use pd.get_dummies)\n",
        "dataset = pd.get_dummies(dataset, columns=['Species'], drop_first=True)\n",
        "dataset.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 Split train / test"
      ],
      "metadata": {
        "id": "gr5ax9yLdib0"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1ZVmnLzlyHX"
      },
      "source": [
        "X_train, X_test = train_test_split(dataset, train_size=0.8, random_state=21)\n",
        "Y_train = X_train.Weight\n",
        "Y_test = X_test.Weight\n",
        "del X_train[\"Weight\"]\n",
        "del X_test[\"Weight\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4 Train your model"
      ],
      "metadata": {
        "id": "Hw0ukaUHdmE6"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFmLYLgodvGM"
      },
      "source": [
        "model = ???\n",
        "model = model.fit(???)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.5 Evaluate your model (here RMSE)"
      ],
      "metadata": {
        "id": "OVkDSB2Wdo1R"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQekPRrUjGWp"
      },
      "source": [
        "Y_pred = model.predict(???)\n",
        "mean_squared_error(Y_test, Y_pred, squared=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2OKTCPYJh0g"
      },
      "source": [
        "### 3.6 Improve your model by tuning SVR hyperparameters\n",
        "We will use the function: sklearn.model_selection.GridSearchCV(estimator, param_grid) where:\n",
        "- estimator: estimator object, svm.SVC() in our case\n",
        "- param_grid: dict with parameters names (string) as keys and lists of parameter settings to try as values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3W4SQBjC4uz"
      },
      "source": [
        "param_grid = {\n",
        "    \"kernel\": [???],\n",
        "    \"C\": [???],\n",
        "    \"gamma\": [???],\n",
        "    \"epsilon\": [???]\n",
        "}\n",
        "grid = GridSearchCV(???, ???, scoring=\"neg_root_mean_squared_error\", cv=???)\n",
        "grid.fit(X_train, Y_train)\n",
        "\n",
        "print(\"We obtained a training RMSE score of:\", abs(grid.best_score_), \"with parameters:\", grid.best_params_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R6hWcj98B4Og"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}